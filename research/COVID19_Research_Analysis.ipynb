{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nrs/SideProjects/COVID19_Research_Analysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set current working directory\n",
    "os.chdir('/home/nrs/SideProjects/COVID19_Research_Analysis/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_paper_data(dirs, papers_info):\n",
    "    for d in dirs:\n",
    "        papers = os.listdir(d)\n",
    "        \n",
    "        for paper in papers:\n",
    "            paper_path = os.path.join(d, paper)\n",
    "            \n",
    "            if os.path.isdir(paper_path):\n",
    "                gather_paper_data([paper_path], papers_info)\n",
    "            else:\n",
    "                with open(paper_path, 'rb') as f:\n",
    "                    file_data = json.load(f)\n",
    "\n",
    "                    paper_id = file_data['paper_id']\n",
    "                    title = file_data['metadata']['title']\n",
    "\n",
    "                    try:\n",
    "                        abstract_paragraphs = file_data['abstract']\n",
    "                    except KeyError:  # Note: this occurs for pmc_json files since none of them have an abstract\n",
    "                        abstract_paragraphs = []\n",
    "                    abstract = []\n",
    "                    for paragraph in abstract_paragraphs:\n",
    "                        abstract.append(paragraph['text'])\n",
    "                    abstract = '\\n'.join(abstract)\n",
    "\n",
    "                    try:\n",
    "                        body_paragraphs = file_data['body_text']\n",
    "                    except KeyError:\n",
    "                        body_paragraphs = []\n",
    "                    body = []\n",
    "                    for paragraph in body_paragraphs:\n",
    "                        body.append(paragraph['text'])\n",
    "                    body = '\\n'.join(body)\n",
    "\n",
    "                    papers_info.append([paper_id, title, abstract, body])\n",
    "\n",
    "    return papers_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_paper(text, keywords):\n",
    "    text = ([word.lower().strip() for word in text.split(' ')])\n",
    "    for keyword in keywords:\n",
    "        keyword_parts = keyword.split(' ')\n",
    "        if any(word in text for word in keyword_parts):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_paper(text, tokenizer):\n",
    "    cleaned_text = []\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    text_doc = nlp(text)\n",
    "    text_tokens = ' '.join([\n",
    "        token.lemma_.lower().strip() for token in text_doc \n",
    "        if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-'\n",
    "    ])\n",
    "    \n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Docs*** <br>\n",
    "Get all papers from directories: arxiv, biorxiv_medrxiv, comm_use_subset, noncomm_use_subset <br>\n",
    "Convert into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27299, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_info = []\n",
    "gather_paper_data(['arxiv', 'biorxiv_medrxiv', 'comm_use_subset', 'noncomm_use_subset'], papers_info)\n",
    "papers_df = pd.DataFrame(papers_info, columns=['paper_id', 'title', 'abstract', 'body'])\n",
    "papers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Docs***<br>\n",
    "Filter in relevant papers depending on the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8293, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_task_keywords = [\n",
    "    'drug', 'patients', 'therapeutic', 'vaccine', 'animal', 'clinical', 'trial', 'prophylaxis', \n",
    "    'prophylactic', 'distribution', 'studies', 'immunity', 'model', 'prioritize', 'efficacy'\n",
    "]\n",
    "treatment_papers_df = papers_df[\n",
    "    papers_df.apply(lambda paper: filter_paper(paper['abstract'], treatment_task_keywords), axis=1)\n",
    "]\n",
    "treatment_papers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Paper Abstracts and Bodies via Multiprocessing for Further Text Mining and NLP Analysis <br>\n",
    "Performance comparison: <br>\n",
    "Cleaning ~8000 abstracts: <br>\n",
    "267 s (apply w/o optimizations); 135 s (threads); 114 s (processes) <br>\n",
    "Cleaning 2000 bodies: <br>\n",
    "667 s (apply w/o optimizations); 293 s (threads); 256 s (processes) <br>\n",
    "Cleaning all bodies: <br>\n",
    "1080 s (processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "treatment_papers_data = dd.from_pandas(treatment_papers_df, npartitions=30)\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time of cleaning paper abstracts (on multiple processes):  108.02366828918457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrs/.local/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "treatment_papers_df['cleaned_abstract'] = treatment_papers_data.map_partitions(\n",
    "    lambda df: df['abstract'].apply(\n",
    "        lambda abstract: clean_paper(abstract, tokenizer)\n",
    "    )\n",
    ").compute(scheduler='processes')\n",
    "end = time.time()\n",
    "\n",
    "print('Exec time of cleaning paper abstracts (on multiple processes): ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time of cleaning paper bodies (on multiple processes):  1085.143699645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrs/.local/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "treatment_papers_df['cleaned_body'] = treatment_papers_data.map_partitions(\n",
    "    lambda df: df['body'].apply(\n",
    "        lambda body: clean_paper(body, tokenizer)\n",
    "    )\n",
    ").compute(scheduler='processes')\n",
    "end = time.time()\n",
    "\n",
    "print('Exec time of cleaning paper bodies (on multiple processes): ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file; don't want to recompute cleaned text\n",
    "treatment_papers_df.to_csv('treatment_papers_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_papers_df = pd.read_csv('treatment_papers_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: What do we know about vaccines or therapeutics? \n",
    "* Effectiveness of drugs being developed and used to treat patients <br>\n",
    "* Potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients <br>\n",
    "* Exploration of use of best animal models and their predictive value for a human vaccine <br>\n",
    "* Capabilities to discover a therapeutic for the disease, and clinical effectiveness studies to discover therapeutics <br>\n",
    "* Alternative models in prioritizing and distributing scarce, newly proven therapeutics and vaccines at scale <br>\n",
    "* Efforts targeted at a universal coronavirus vaccine <br>\n",
    "* Efforts to develop animal models and standardize challenge studies <br>\n",
    "* Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers <br>\n",
    "* Approaches to evaluate risk for enhanced disease after vaccination <br>\n",
    "* Assays to evaluate vaccine immune response and process development for vaccines <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Strategy: Dumb Filtering/Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_papers_df = treatment_papers_df[treatment_papers_df['cleaned_abstract'].str.contains('drug')]\n",
    "drug_abstracts = drug_papers_df['abstract'].values  # numpy arrays\n",
    "drug_bodies = drug_papers_df['body'].values  # numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The appearance of a new dangerous and contagious disease requires the development of a drug therapy faster than what is foreseen by usual mechanisms\n",
      "Sentence:   Many drug therapy developments consist in investigating through different clinical trials the effects of different specific drug combinations by delivering it into a test group of ill patients, meanwhile a placebo treatment is delivered to the remaining ill patients, known as the control group\n",
      "Sentence:   We compare the above technique to a new technique in which all patients receive a different and reasonable combination of drugs and use this outcome to feed a Neural Network\n",
      "Sentence:   By averaging out fluctuations and recognizing different patient features, the Neural Network learns the pattern that connects the patients initial state to the outcome of the treatments and therefore can predict the best drug therapy better than the above method\n",
      "Sentence:   In contrast to many available works, we do not study any detail of drugs composition nor interaction, but instead pose and solve the problem from a phenomenological point of view, which allows us to compare both methods\n"
     ]
    }
   ],
   "source": [
    "for abstract in drug_abstracts:\n",
    "    sentences = abstract.split('.')\n",
    "    for sentence in sentences:\n",
    "        if 'drug' in sentence:\n",
    "            print('Sentence: ', sentence)\n",
    "    break  # Stop at 1 abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Strategy: Productionize Elasticsearch implementation\n",
    "We'll get a blazing fast implementation with easily pluggable search and text analysis functions from Elasticsearch. <br>\n",
    "It's also excellent practice for deploying backends as lightweight Docker containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database indexes are data structures used to speed up queries and retrievals on database records. These indexes will store a field and a reference to its corresponding record in a data structure like B-Trees. <br>\n",
    "B-Trees are especially suited for fast search operations even after many insertions and deletions (due to it storing keys in sorted order and fast rebalancing operations). <br>\n",
    "Insert, Delete, Search: O(t log_t(n)) where n = # keys, t = # keys in node, # disk operations = O(log_t(n)) <br> <br>\n",
    "\n",
    "Elasticsearch, on the other hand, uses **inverted indexes**. Each word is indexed and points back to document(s) in which it was found and its location within the document. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start elasticsearch container (i.e. server) by building docker-compose.yml file: ```sudo `which docker-compose` up -d --build```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index covid19_papers already exists; continue with execution:  RequestError(400, 'resource_already_exists_exception', 'index [covid19_papers/X4wA09UQRQSfmc3oxjUUig] already exists')\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, RequestError\n",
    "\n",
    "es = Elasticsearch(hosts=[\"localhost\"])\n",
    "try:\n",
    "    es.indices.create(index='covid19_papers')\n",
    "except RequestError as e:\n",
    "    print(\"Index covid19_papers already exists; continue with execution: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_to_actions(df, index, data_type):\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        yield('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (index, data_type))\n",
    "        yield(json.dumps(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 'covid19_papers'\n",
    "DATA_TYPE = 'record'\n",
    "\n",
    "def upload_papers_to_es_idx(es_idx, data_type):\n",
    "    chunk_size = 1000\n",
    "    idx = 0\n",
    "    while idx < papers_df.shape[0]:\n",
    "        if idx + chunk_size < papers_df.shape[0]:\n",
    "            max_idx = idx + chunk_size\n",
    "        else:\n",
    "            max_idx = papers_df.shape[0]\n",
    "        print('Range: ', idx, max_idx)\n",
    "        \n",
    "        r = es.bulk(rec_to_actions(papers_df[idx:max_idx], es_idx, data_type))\n",
    "        print(not r[\"errors\"])\n",
    "        \n",
    "        idx = max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which local spread has already occurred and testing availability is delayed. Since stay-at-home orders reduce infection growth rates, early implementation when infection counts are still low would be most beneficial.\n",
      "Funding None.\n",
      "\n",
      "\n",
      "\n",
      "c0b561d09eef775c862a1bee9559f1c707f42e97 | Evidence of economic segregation from mobility lockdown during COVID-19 epidemic: In response to the COVID-19 pandemic, National governments have applied lockdown restrictions to reduce the infection rate. We perform a massive analysis on near real-time Italian data provided by Facebook to investigate how lockdown strategies affect economic conditions of individuals and local governments. We model the change in mobility as an exogenous shock similar to a natural disaster. We identify two ways through which mobility restrictions affect Italian citizens. First, we find that the impact of lockdown is stronger in municipalities with higher fiscal capacity. Second, we find a segregation effect, since mobility restrictions are stronger in municipalities for which inequality is higher and where individuals have lower income per capita.\n",
      "\n",
      "\n",
      "\n",
      "7b72de65011af1999b1beea472cca8d95b25abb8 | CovidSens: A Vision on Reliable Social Sensing based Risk Alerting Systems for COVID-19 Spread: With the spiraling pandemic of the Coronavirus Disease 2019 , it has becoming inherently important to disseminate accurate and timely information about the disease. Due to the ubiquity of Internet connectivity and smart devices, social sensing is emerging as a dynamic sensing paradigm to collect real-time observations from online users. In this vision paper we propose CovidSens, the concept of social-sensing-based risk alerting systems to notify the general public about the COVID-19 spread. The CovidSens concept is motivated by two recent observations: 1) people have been actively sharing their state of health and experience of the COVID-19 via online social media, and 2) official warning channels and news agencies are relatively slower than people reporting their observations and experiences about COVID-19 on social media. We anticipate an unprecedented opportunity to leverage the posts generated by the social media users to build a real-time analytic system for gathering and circulating vital information of the COVID-19 propagation. Specifically, the vision of CovidSens attempts to answer the questions of: how to track the spread of the COVID-19? How to distill reliable information about the disease with the coexistence of prevailing rumors and misinformation in the social media? How to inform the general public about the latest state of the spread timely and effectively and alert them to remain prepared? In this vision paper, we discuss the roles of CovidSens and identify the potential challenges in implementing reliable social-sensing-based risk alerting systems. We envision that approaches originating from multiple disciplines (e.g. estimation theory, machine learning, constrained optimization) can be effective in addressing the challenges. Finally, we outline a few research directions for future work in CovidSens.\n",
      "\n",
      "\n",
      "\n",
      "709f2dbfc5ee48f65ca835ef66ec9e3dc47746b1 | Flattening the Curve: Insights From Queueing Theory: The worldwide outbreak of the coronavirus was first identified in 2019 in Wuhan, China. Since then, the disease has spread worldwide. As it currently spreading in the United States, policy makers, public health officials and citizens are racing to understand the impact of this virus on the United States healthcare system. They fear that the rapid influx of patients will overwhelm the healthcare system leading to unnecessary fatalities. Most countries and states in America have introduced mitigation strategies, * Corresponding Author such as social distancing, to decrease the rate of newly infected people, i.e. flattening the curve.\n",
      "In this paper, we analyze the time evolution of the number of people hospitalized due to the coronavirus using the methods of queueing theory. Given that the rate of new infections varies over time as the pandemic evolves, we model the number of coronavirus patients as a dynamical system based on the theory of infinite server queues with non-stationary Poisson arrival rates. With this model we are able to quantify how flattening the curve affects the peak demand for hospital resources. This allows us to characterize how aggressively society must flatten the curve in order to avoid overwhelming the capacity of healthcare system. We also demonstrate how flattening the curve impacts the elapsed time between the peak rate of hospitalizations and the time of the peak demand for the hospital resources. Finally, we present empirical evidence from China, South Korea, Italy and the United States that supports the insights from the model.\n",
      "\n",
      "\n",
      "\n",
      "cd5d56f9860dabbcb9d1ef512d8009255db7200e | COVID-19 in Africa -outbreak despite interventions?: In Africa, while most countries report some COVID-19 cases, the fraction of reported patients is low, with about 20 000 cases compared to the more than 2.3 million cases reported globally as of April 18, 2020.\n",
      "Few African countries have reported case numbers above one thousand, with South Africa reporting 3 034 cases being hit hardest in Sub-Saharan Africa. Several African countries, especially South Africa, have already taken strong non-pharmaceutical interventions that include physical distancing, restricted economic, educational and leisure activities and reduced human mobility options. The required strengths and overall effectiveness of such interventions, however, are debated because of simultaneous but opposing interests in most African countries: strongly limited health care capacities and testing capabilities largely conflict with pressured national economies and socio-economic hardships on the individual level, limiting compliance to intervention targets. Here we investigate implications of interventions on the COVID-19 outbreak dynamics, focusing on South Africa before and after the national lockdown enacted on March 27, 2020. Our analysis shows that initial exponential growth of existing case numbers is consistent with doubling times of about 2.5 days. After lockdown, the growth remains exponential, now with doubling times of 18 days, but still in contrast to subexponential growth reported for Hubei/China after lockdown. Moreover, a scenario analysis of a computational data-driven agent based mobility model for the Nelson Mandela Bay Municipality (with 1.14 million inhabitants) hints that keeping current levels of intervention measures and compliance until the end of April is of insufficient length and still too weak, too unspecific or too inconsistently complied with to not overload local intensive care capacity. Yet, enduring, slightly stronger, more specific interventions combined with sufficient compliance may constitute a viable option for interventions for regions in South Africa and potentially for large parts of the African continent.\n",
      "\n",
      "\n",
      "\n",
      "d80e378c031591d02641b545a2262e50fe80bb89 | Strong correlations between power-law growth of COVID-19 in four continents and the inefficiency of soft quarantine strategies: In this work we analyse the growth of the cumulative number of confirmed infected cases by the COVID-19 until March 27 th , 2020, from countries of Asia, Europe, North and South America. Our results show (i) that power-law growth is observed for all countries; (ii) by using the distance correlation, that the power-law curves between countries are statistically highly correlated, suggesting the universality of such curves around the World; and (iii) that soft quarantine strategies are inefficient to flatten the growth curves. Furthermore, we present a model and strategies which allow the government to reach the flattening of the power-law curves. We found that, besides the social distance of individuals, of well known relevance, the strategy of identifying and isolating infected individuals in a large daily rate, can help to flatten the power-laws. These are essentially the strategies used in the Republic of Korea. The high correlation between the power-law curves of different countries strongly indicate that the government containment measures can be applied with success around the whole World. These measures must be scathing and applied as soon as possible.\n",
      "\n",
      "\n",
      "\n",
      "067e53fab4bdec6e6dd07365d38f7e8b3be9c688 | Using generalized logistics regression to forecast population infected by Covid-19: In this work, a proposal to forecast the populations using generalized logistics regression curve fitting is presented. This type of curve is used to study population growth, in this case population of people infected with the Covid-19 virus; and it can also be used to approximate the survival curve used in actuarial and similar studies.\n",
      "\n",
      "\n",
      "\n",
      "fa4c0136b5f96f629bab51b569a3eb4f24b0db0f | Total Variation Regularization for Compartmental Epidemic Models with Time-varying Dynamics: Traditional methods to infer compartmental epidemic models with time-varying dynamics can only capture continuous changes in the dynamic. However, many changes are discontinuous due to sudden interventions, such as city lockdown and opening of field hospitals. To model the discontinuities, this study introduces the tool of total variation regularization, which regulates the temporal changes of the dynamic parameters, such as the transmission rate. To recover the ground truth dynamic, this study designs a novel yet straightforward optimization algorithm, dubbed iterative Nelder-Mead, which repeatedly applies the Nelder-Mead algorithm. Experiments on the simulated data show that the proposed approach can qualitatively reproduce the discontinuities of the underlying dynamics. To extend this research to real data as well as to help researchers worldwide to fight against COVID-19, the author releases his research platform as an open-source package.\n",
      "\n",
      "\n",
      "\n",
      "2c837018d332088b3b45f0b6189d1df3cb53064d | USING ALTMETRICS FOR DETECTING IMPACTFUL RESEARCH IN QUASI-ZERO-DAY TIME-WINDOWS: THE CASE OF COVID-19 A PREPRINT: \n",
      "\n",
      "\n",
      "\n",
      "b923a8fc7d406d0f149ecd5293d1a72dec7ee526 | An alternating lock-down strategy for sustainable mitigation of COVID-19: Lacking a drug or vaccine, our current strategy to contain the COVID-19 pandemic is by means of social distancing, specifically mobility restrictions and lock-downs. Such measures impose a hurtful toll on the economy, and are difficult to sustain for extended periods. The challenge is that selective isolation of the sick, an often viable and effective strategy, is insufficient against COVID-19, due to its relatively long incubation period, in which exposed individuals experience no symptoms, but still contribute to the spread. Here we propose an alternating lock-down strategy, in which at every instance, half of the population remains under lock-down while the other half continues to be active, maintaining a routine of weekly succession between activity and lock-down. All symptomatic individuals continue to remain in isolation. Under this regime, if an individual was exposed during their active week, by the time they complete their lock-down they will already begin to exhibit symptoms. Hence this strategy isolates the majority of exposed individuals during their asymptomatic phase. We find that this strategy not only overcomes the pandemic, but also allows for some level of flexibility, withstanding a fraction of defectors or essential workers that remain continuously active. We examine our strategy based on current epidemiological models with parameters relevant for COVID-19. We wish, however, following this communication, to further test and fine-tune our scheme based more refined data, and assess its actual effectiveness.\n",
      "Note from the authors. In light of the imminence of the matter, we have decided to publish this report, even in its preliminary form, forgoing the scientific instinct of scrutiny and reservedness. We will continue to refine and retest our results and update this report on the go. We also welcome feedback, comments, questions or advice that can help further test or improve our proposed strategy.\n",
      "As we battle the virulent spread of COVID-19 we seek efficient strategies to mitigate its global impact. Lacking therapeutic interventions, such as drugs or vaccines, we resort to social distancing, aimed primarily at lowering the reproduction rate R 0 and flattening the curve, i.e. reducing the total number of infected individuals, while spreading their infection over an extended period [1-3]. Such measures are designed to avoid shocking the health-care system, reducing the medical burden, and slowing its onset, hence keeping it within the bounds of the system's capacity. To achieve this many countries have imposed social restrictions [3], from complete lock-downs, to severe mobility constraints, indeed, slowing down the viral propagation, but at the same time, taking a sever toll on social and economic stability. Most current projections on COVID-19 indicate that such social distancing policies must be put in place for extended periods (typically months) to avoid reemergence of the epidemic once lifted [4] . This, however, may be unsustainable, as individual social and economic needs will, at some point surpass the perceived risk of the pandemic.\n",
      "Another challenge, specific to COVID-19, is its incubation period, estimated at ∼ 5 days on average\n",
      "\n",
      "\n",
      "\n",
      "bddf336f4c53ce53c22a54de37aa00c8b723a291 | Weather-inspired ensemble-based probabilistic prediction of COVID-19: The objective of this work is to predict the spread of COVID-19 starting from observed data, using a forecast method inspired by probabilistic weather prediction systems operational today.\n",
      "Results show that this method works well for China: on day 25 we could have predicted well the outcome for the next 35 days. The same method has been applied to Italy and South Korea, and forecasts for the forthcoming weeks are included in this work. For Italy, forecasts based on data collected up to today (24 March) indicate that number of observed cases could grow from the current value of 69,176, to between 101k-180k, with a 50% probability of being between 110k-135k. For South Korea, it suggests that the number of observed cases could grow from the current value of 9,018 (as of the 23rd of March), to values between 8,500 and 9,300, with a 50% probability of being between 8,700 and 8,900.\n",
      "We conclude by suggesting that probabilistic disease prediction systems are possible and could be developed following key ideas and methods from weather forecasting. Having access to skilful daily updated forecasts could help taking better informed decisions on how to manage the spread of diseases such as COVID-19.\n",
      "\n",
      "\n",
      "\n",
      "f0d58f10263d2a5a7c87f0a73c0f38b1af2e67be | Optimal resource diffusion for suppressing disease spreading in multiplex networks: Resource diffusion is an ubiquitous phenomenon, but how it impacts epidemic spreading has received little study. We propose a model that couples epidemic spreading and resource diffusion in multiplex networks. The spread of disease in a physical contact layer and the recovery of the infected nodes are both strongly dependent upon resources supplied by their counterparts in the social layer. The generation and diffusion of resources in the social layer are in turn strongly dependent upon the state of the nodes in the physical contact layer. Resources diffuse preferentially or randomly in this model. To quantify the degree of preferential diffusion, a bias parameter that controls the resource diffusion is proposed. We conduct extensive simulations and find that the preferential resource diffusion can change phase transition type of the fraction of infected nodes. When the degree of interlayer correlation is below a critical value, increasing the bias parameter changes the phase transition from double continuous to single continuous. When the degree of interlayer correlation is above a critical value, the phase transition changes from multiple continuous to first discontinuous and then to hybrid. We find hysteresis loops in the phase transition. We also find that there is an optimal resource strategy at each fixed degree of interlayer correlation where the threshold reaches a maximum and under which the disease can be maximally suppressed. In addition, the optimal controlling parameter increases as the degree of inter-layer correlation increases.\n",
      "\n",
      "\n",
      "\n",
      "f32610fc5bca19a9e0890eb1e868d057397762ad | Recognition of potential Covid-19 drug treatments through study of existing protein-drug structures: an analysis of thermodynamically active residues: We report results of our study of approved drugs as potential treatments for COVID-19 based on the application of various bioinformatics predictive methods. The drugs studied include hydroxychloroquine, ivermectin, remdesivir and α-difluoromethylornithine (DMFO). Our results indicate that these small drug molecules selectively bind to thermodynamically active residues on protein surfaces, and that some prefer hydrophobic over other active sites. Our approach is not restricted to viruses and can facilitate rational drug design, as well as improve our understanding of molecular interactions, in general.\n",
      "\n",
      "\n",
      "\n",
      "855e9a1e74eceea68ad316023622114cfadf026b | In Silico Investigations on the Potential Inhibitors for COVID-19 Protease *: A novel strain of coronavirus, namely, COVID-19 has been identified in Wuhan city of China in December 2019. There are no specific therapies available and investigations regarding the treatment of the COVID-19 are still lacking. This prompted us to perform a preliminary in silico study on the COVID-19 protease with anti-malarial compounds in the search of potential inhibitor. We have calculated log P and log S values in addition to molecular docking and PASS predictions. Among the seven studied compounds, mepacrine appears as the potential inhibitor of the COVID-19 followed by chloroquine, hydroxychloroquine and phomarin. Therefore, these anti-malarial drugs may be potential drug candidate for the treatment of this novel coronavirus.\n",
      "A detailed analysis on these inhibitors is currently in progress and clinical studies are invited to investigate their potential medicinal use for the COVID-19.\n",
      "\n",
      "\n",
      "\n",
      "4cc3622caaba4cf5c90c64924ace1d2c3e0b40e1 | An Ensemble Approach to Predicting the Impact of Vaccination on Rotavirus Disease in Niger: Recently developed vaccines provide a new way of controlling rotavirus in sub-Saharan Africa.\n",
      "Models for the transmission dynamics of rotavirus are critical both for estimating current burden from imperfect surveillance and for assessing potential effects of vaccine intervention strategies.\n",
      "We examine rotavirus infection in the Maradi area in southern Niger using hospital surveillance data provided by Epicentre collected over two years. Additionally, a cluster survey of households in the region allows us to estimate the proportion of children with diarrhea who consulted at a health structure. Model fit and future projections are necessarily particular to a given model; thus, where there are competing models for the underlying epidemiology an ensemble approach can account for that uncertainty. We compare our results across several variants of Susceptible-Infectious-Recovered (SIR) compartmental models to quantify the impact of modeling assumptions on our estimates. Model-specific parameters are estimated by Bayesian inference using Markov chain Monte Carlo. We then use Bayesian model averaging to generate ensemble estimates of the current dynamics, including estimates of R0, the burden of infection in the region, as well as the impact of vaccination on both the short-term dynamics and the long-term reduction of rotavirus incidence under varying levels of coverage. The ensemble of models predicts that the current burden of severe rotavirus disease is 2.9 to 4.1% of the population each year and that a 2-dose vaccine schedule achieving 70% coverage could reduce burden by 37-43%.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = es.search(\n",
    "    index=\"covid19_papers\", \n",
    "    body={\n",
    "        \"from\": 0,\n",
    "        \"size\": 20,\n",
    "        \"query\": {\"match_all\": {}}\n",
    "    }\n",
    ")\n",
    "for hit in res['hits']['hits']:\n",
    "    print(\"%(paper_id)s | %(title)s: %(abstract)s\" % hit[\"_source\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit4726871660824b01b6df5737252367af"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
