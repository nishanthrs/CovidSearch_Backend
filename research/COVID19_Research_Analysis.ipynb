{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nrs/SideProjects/COVID19_Research_Analysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set current working directory\n",
    "os.chdir('/home/nrs/SideProjects/COVID19_Research_Analysis/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_paper_data(dirs, papers_info):\n",
    "    for d in dirs:\n",
    "        papers = os.listdir(d)\n",
    "        \n",
    "        for paper in papers:\n",
    "            paper_path = os.path.join(d, paper)\n",
    "            \n",
    "            if os.path.isdir(paper_path):\n",
    "                gather_paper_data([paper_path], papers_info)\n",
    "            else:\n",
    "                with open(paper_path, 'rb') as f:\n",
    "                    file_data = json.load(f)\n",
    "\n",
    "                    paper_id = file_data['paper_id']\n",
    "                    title = file_data['metadata']['title']\n",
    "\n",
    "                    try:\n",
    "                        abstract_paragraphs = file_data['abstract']\n",
    "                    except KeyError:  # Note: this occurs for pmc_json files since none of them have an abstract\n",
    "                        abstract_paragraphs = []\n",
    "                    abstract = []\n",
    "                    for paragraph in abstract_paragraphs:\n",
    "                        abstract.append(paragraph['text'])\n",
    "                    abstract = '\\n'.join(abstract)\n",
    "\n",
    "                    try:\n",
    "                        body_paragraphs = file_data['body_text']\n",
    "                    except KeyError:\n",
    "                        body_paragraphs = []\n",
    "                    body = []\n",
    "                    for paragraph in body_paragraphs:\n",
    "                        body.append(paragraph['text'])\n",
    "                    body = '\\n'.join(body)\n",
    "\n",
    "                    papers_info.append([paper_id, title, abstract, body])\n",
    "\n",
    "    return papers_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_paper(text, keywords):\n",
    "    text = ([word.lower().strip() for word in text.split(' ')])\n",
    "    for keyword in keywords:\n",
    "        keyword_parts = keyword.split(' ')\n",
    "        if any(word in text for word in keyword_parts):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_paper(text, tokenizer):\n",
    "    cleaned_text = []\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    text_doc = nlp(text)\n",
    "    text_tokens = ' '.join([\n",
    "        token.lemma_.lower().strip() for token in text_doc \n",
    "        if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-'\n",
    "    ])\n",
    "    \n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all papers from directories: arxiv, biorxiv_medrxiv, comm_use_subset, noncomm_use_subset <br>\n",
    "Convert into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27299, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_info = []\n",
    "gather_paper_data(['arxiv', 'biorxiv_medrxiv', 'comm_use_subset', 'noncomm_use_subset'], papers_info)\n",
    "papers_df = pd.DataFrame(papers_info, columns=['paper_id', 'title', 'abstract', 'body'])\n",
    "papers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter in relevant papers depending on the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8293, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_task_keywords = [\n",
    "    'drug', 'patients', 'therapeutic', 'vaccine', 'animal', 'clinical', 'trial', 'prophylaxis', \n",
    "    'prophylactic', 'distribution', 'studies', 'immunity', 'model', 'prioritize', 'efficacy'\n",
    "]\n",
    "treatment_papers_df = papers_df[\n",
    "    papers_df.apply(lambda paper: filter_paper(paper['abstract'], treatment_task_keywords), axis=1)\n",
    "]\n",
    "treatment_papers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Paper Abstracts and Bodies via Multiprocessing for Further Text Mining and NLP Analysis <br>\n",
    "Performance comparison: <br>\n",
    "Cleaning ~8000 abstracts: <br>\n",
    "267 s (apply w/o optimizations); 135 s (threads); 114 s (processes) <br>\n",
    "Cleaning 2000 bodies: <br>\n",
    "667 s (apply w/o optimizations); 256 s (processes); 293 s (threads) <br>\n",
    "Cleaning all bodies: <br>\n",
    "1080 s (processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "treatment_papers_data = dd.from_pandas(treatment_papers_df, npartitions=30)\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time of cleaning paper abstracts (on multiple processes):  107.33939123153687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrs/.local/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "treatment_papers_df['cleaned_abstract'] = treatment_papers_data.map_partitions(\n",
    "    lambda df: df['abstract'].apply(\n",
    "        lambda abstract: clean_paper(abstract, tokenizer)\n",
    "    )\n",
    ").compute(scheduler='processes')\n",
    "end = time.time()\n",
    "\n",
    "print('Exec time of cleaning paper abstracts (on multiple processes): ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time of cleaning paper bodies (on multiple processes):  1080.5138938426971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrs/.local/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "treatment_papers_df['cleaned_body'] = treatment_papers_data.map_partitions(\n",
    "    lambda df: df['body'].apply(\n",
    "        lambda body: clean_paper(body, tokenizer)\n",
    "    )\n",
    ").compute(scheduler='processes')\n",
    "end = time.time()\n",
    "\n",
    "print('Exec time of cleaning paper bodies (on multiple processes): ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file; don't want to recompute cleaned text\n",
    "treatment_papers_df.to_csv('treatment_papers_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_papers_df = pd.read_csv('treatment_papers_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: What do we know about vaccines or therapeutics? \n",
    "* Effectiveness of drugs being developed and used to treat patients <br>\n",
    "* Potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients <br>\n",
    "* Exploration of use of best animal models and their predictive value for a human vaccine <br>\n",
    "* Capabilities to discover a therapeutic for the disease, and clinical effectiveness studies to discover therapeutics <br>\n",
    "* Alternative models in prioritizing and distributing scarce, newly proven therapeutics and vaccines at scale <br>\n",
    "* Efforts targeted at a universal coronavirus vaccine <br>\n",
    "* Efforts to develop animal models and standardize challenge studies <br>\n",
    "* Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers <br>\n",
    "* Approaches to evaluate risk for enhanced disease after vaccination <br>\n",
    "* Assays to evaluate vaccine immune response and process development for vaccines <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Strategy: Dumb Filtering/Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_papers_df = treatment_papers_df[treatment_papers_df['cleaned_abstract'].str.contains('drug')]\n",
    "drug_abstracts = drug_papers_df['abstract'].values  # numpy arrays\n",
    "drug_bodies = drug_papers_df['body'].values  # numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The appearance of a new dangerous and contagious disease requires the development of a drug therapy faster than what is foreseen by usual mechanisms\n",
      "Sentence:   Many drug therapy developments consist in investigating through different clinical trials the effects of different specific drug combinations by delivering it into a test group of ill patients, meanwhile a placebo treatment is delivered to the remaining ill patients, known as the control group\n",
      "Sentence:   We compare the above technique to a new technique in which all patients receive a different and reasonable combination of drugs and use this outcome to feed a Neural Network\n",
      "Sentence:   By averaging out fluctuations and recognizing different patient features, the Neural Network learns the pattern that connects the patients initial state to the outcome of the treatments and therefore can predict the best drug therapy better than the above method\n",
      "Sentence:   In contrast to many available works, we do not study any detail of drugs composition nor interaction, but instead pose and solve the problem from a phenomenological point of view, which allows us to compare both methods\n"
     ]
    }
   ],
   "source": [
    "for abstract in drug_abstracts:\n",
    "    sentences = abstract.split('.')\n",
    "    for sentence in sentences:\n",
    "        if 'drug' in sentence:\n",
    "            print('Sentence: ', sentence)\n",
    "    break  # Stop at 1 abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Strategy: Productionize Elasticsearch implementation\n",
    "We'll get a blazing fast implementation with easily pluggable search and text analysis functions from Elasticsearch. <br>\n",
    "It's also excellent practice for deploying backends as lightweight Docker containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database indexes are data structures used to speed up queries and retrievals on database records. These indexes will store a field and a reference to its corresponding record in a data structure like B-Trees. <br>\n",
    "B-Trees are especially suited for fast search operations even after many insertions and deletions (due to it storing keys in sorted order and fast rebalancing operations). <br>\n",
    "Insert, Delete, Search: O(t log_t(n)) where n = # keys, t = # keys in node, # disk operations = O(log_t(n)) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch, on the other hand, uses **inverted indexes**. Each word is indexed and points back to document(s) in which it was found and its location within the document. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'root_cause': [{'type': 'resource_already_exists_exception',\n",
       "    'reason': 'index [covid19_papers/X4wA09UQRQSfmc3oxjUUig] already exists',\n",
       "    'index_uuid': 'X4wA09UQRQSfmc3oxjUUig',\n",
       "    'index': 'covid19_papers'}],\n",
       "  'type': 'resource_already_exists_exception',\n",
       "  'reason': 'index [covid19_papers/X4wA09UQRQSfmc3oxjUUig] already exists',\n",
       "  'index_uuid': 'X4wA09UQRQSfmc3oxjUUig',\n",
       "  'index': 'covid19_papers'},\n",
       " 'status': 400}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(hosts=[\"localhost\"])\n",
    "# es.indices.create(index='covid19_papers', ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_to_actions(df, index, data_type):\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        yield('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (index, data_type))\n",
    "        yield(json.dumps(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range:  0 1000\n",
      "True\n",
      "Range:  1000 2000\n",
      "True\n",
      "Range:  2000 3000\n",
      "True\n",
      "Range:  3000 4000\n",
      "True\n",
      "Range:  4000 5000\n",
      "True\n",
      "Range:  5000 6000\n",
      "True\n",
      "Range:  6000 7000\n",
      "True\n",
      "Range:  7000 8000\n",
      "True\n",
      "Range:  8000 9000\n",
      "True\n",
      "Range:  9000 10000\n",
      "True\n",
      "Range:  10000 11000\n",
      "True\n",
      "Range:  11000 12000\n",
      "True\n",
      "Range:  12000 13000\n",
      "True\n",
      "Range:  13000 14000\n",
      "True\n",
      "Range:  14000 15000\n",
      "True\n",
      "Range:  15000 16000\n",
      "True\n",
      "Range:  16000 17000\n",
      "True\n",
      "Range:  17000 18000\n",
      "True\n",
      "Range:  18000 19000\n",
      "True\n",
      "Range:  19000 20000\n",
      "True\n",
      "Range:  20000 21000\n",
      "True\n",
      "Range:  21000 22000\n",
      "True\n",
      "Range:  22000 23000\n",
      "True\n",
      "Range:  23000 24000\n",
      "True\n",
      "Range:  24000 25000\n",
      "True\n",
      "Range:  25000 26000\n",
      "True\n",
      "Range:  26000 27000\n",
      "True\n",
      "Range:  27000 27299\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "INDEX = 'covid19_papers'\n",
    "DATA_TYPE = 'record'\n",
    "\n",
    "chunk_size = 1000\n",
    "idx = 0\n",
    "while idx < papers_df.shape[0]:\n",
    "    if idx + chunk_size < papers_df.shape[0]:\n",
    "        max_idx = idx + chunk_size\n",
    "    else:\n",
    "        max_idx = papers_df.shape[0]\n",
    "    print('Range: ', idx, max_idx)\n",
    "    \n",
    "    r = es.bulk(rec_to_actions(papers_df[idx:max_idx], INDEX, DATA_TYPE))\n",
    "    print(not r[\"errors\"])\n",
    "    \n",
    "    idx = max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34e105872c774ec5f4d8c06f0c1b80b370cdc111 | COVID-19 Chest CT Image Segmentation - A Deep Convolutional Neural Network Solution: Objective: A novel coronavirus disease 2019 was detected and has spread rapidly across various countries around the world since the end of the year 2019, Computed Tomography (CT) images have been used as a crucial alternative to the time-consuming RT-PCR test. However, pure manual segmentation of CT images faces a serious challenge with the increase of suspected cases, resulting in urgent requirements for accurate and automatic segmentation of COVID-19 infections. Unfortunately, since the imaging characteristics of the COVID-19 infection are diverse and similar to the backgrounds, existing medical image segmentation methods cannot achieve satisfactory performance. Methods: In this work, we try to establish a new deep convolutional neural network tailored for segmenting the chest CT images with COVID-19 infections. We firstly maintain a large and new chest CT image dataset consisting of 21,658 annotated chest CT images from 861 patients with confirmed COVID-19. Inspired by the observation that the boundary of the infected lung can be enhanced by adjusting the global intensity, in the proposed deep CNN, we introduce a feature variation block which adaptively adjusts the global properties of the features for segmenting COVID-19 infection. The proposed FV block can enhance the capability of feature representation effectively and adaptively for diverse cases. We fuse features at different scales by proposing Progressive Atrous Spatial Pyramid Pooling to handle the sophisticated infection areas with diverse appearance and shapes. Results: The proposed method achieves the stateof-the-art performance. Dice similarity coefficients are 0.987 and 0.726 for lung and COVID-19 segmentation, respectively. Conclusion: We conducted experiments on the data collected in China and Germany and show that the proposed deep CNN can produce impressive performance effectively. Significance: The proposed network enhances the segmentation ability of the COVID-19 infection, makes the connection with other techniques and contributes to the development of remedying COVID-19 infection.\n",
      "90742f2748824fc2d9e3c8bd499f0e51c68b2b38 | A Machine Learning alternative to placebo-controlled clinical trials upon new diseases: A primer: The appearance of a new dangerous and contagious disease requires the development of a drug therapy faster than what is foreseen by usual mechanisms. Many drug therapy developments consist in investigating through different clinical trials the effects of different specific drug combinations by delivering it into a test group of ill patients, meanwhile a placebo treatment is delivered to the remaining ill patients, known as the control group. We compare the above technique to a new technique in which all patients receive a different and reasonable combination of drugs and use this outcome to feed a Neural Network. By averaging out fluctuations and recognizing different patient features, the Neural Network learns the pattern that connects the patients initial state to the outcome of the treatments and therefore can predict the best drug therapy better than the above method. In contrast to many available works, we do not study any detail of drugs composition nor interaction, but instead pose and solve the problem from a phenomenological point of view, which allows us to compare both methods. Although the conclusion is reached through mathematical modeling and is stable upon any reasonable model, this is a proof-of-concept that should be studied within other expertises before confronting a real scenario. All calculations, tools and scripts have been made open source for the community to test, modify or expand it. Finally it should be mentioned that, although the results presented here are in the context of a new disease in medical sciences, these are useful for any field that requires a experimental technique with a control group.\n",
      "beafcc134bb96b4c1898cbb63be513d951f82b95 | Coronavirus (COVID-19) Classification using Deep Features Fusion and Ranking Technique: emerged towards the end of 2019. World Health Organization (WHO) was identified it as a global epidemic. Consensus occurred in the opinion that using Computerized Tomography (CT) techniques for early diagnosis of pandemic disease gives both fast and accurate results. It was stated by expert radiologists that COVID-19 displays different behaviours in CT images. In this study, a novel method was proposed as fusing and ranking deep features to detect COVID-19 in early phase. 16x16 (Subset-1) and 32x32 (Subset-2) patches were obtained from 150 CT images to generate sub-datasets. Within the scope of the proposed method, 3000 patch images have been labelled as CoVID-19 and No finding for using in training and testing phase. Feature fusion and ranking method have been applied in order to increase the performance of the proposed method. Then, the processed data was classified with a Support Vector Machine (SVM). According to other pre-trained Convolutional Neural Network (CNN) models used in transfer learning, the proposed method shows high performance on Subset-2 with 98.27% accuracy, 98.93% sensitivity, 97.60% specificity, 97.63% precision, 98.28% F1-score and 96.54% Matthews Correlation Coefficient (MCC) metrics.\n",
      "5e2874a4424ba10de42e640eb17cfdb283a4f096 | Neural Network aided quarantine control model estimation of COVID spread in Wuhan, China: In a move described as unprecedented in public health history, starting 24 January 2020, China imposed quarantine and isolation restrictions in Wuhan, a city of more than 10 million people. This raised the question: is mass quarantine and isolation effective as a social tool in addition to its scientific use as a medical tool? In an effort to address this question, using a epidemiological model driven approach augmented by machine learning, we show that the quarantine and isolation measures implemented in Wuhan brought down the effective reproduction number R(t) of the CoVID-19 spread from R(t) > 1 to R(t) < 1 within a month after the imposition of quarantine control measures in Wuhan, China. This ultimately resulted in a stagnation phase in the infected case count in Wuhan. Our results indicate that the strict public health policies implemented in Wuhan may have played a crucial role in halting down the spread of infection and such measures should potentially be implemented in other highly affected countries such as South Korea, Italy and Iran to curtail spread of the disease. Finally, our forecasting results predict a stagnation in the quarantine control measures implemented in Wuhan towards the end of March 2020; this would lead to a subsequent stagnation in the effective reproduction number at R(t) < 1. We warn that immediate relaxation of the quarantine measures in Wuhan may lead to a relapse in the infection spread and a subsequent increase in the effective reproduction number to R(t) > 1. Thus, it may be wise to relax quarantine measures after sufficient time has elapsed, during which maximum of the quarantined/isolated individuals are recovered.\n",
      "(*It should be noted when we mention quarantine subsequently in the paper, we mean the infected people who are quarantined and isolated and cannot lead to an infection in a susceptible person.)\n",
      "66fc936b851404690bd89b28316bed4e4858ebe0 | The modelling of COVID19 pathways sheds light on mechanisms, opportunities and on controversial interpretations of medical treatments. v2: The new coronavirus (2019-nCoV or SARS-CoV2), inducing the current pandemic disease and causing pneumoniae in humans, is dramatically increasing in epidemic scale since its first appearance in Wuhan, China, in December 2019. The first infection from epidemic coronaviruses in 2003 fostered the spread of an overwhelming amount of related scientific efforts. The manifold aspects that have been raised, as well as their redundancy offer precious information that has been underexploited and needs to be critically reevaluated, appropriately used and offered to the whole community, from scientists, to medical doctors, stakeholders and common people. These efforts will favour a holistic view on the comprehension, prevention and development of strategies (pharmacological, clinical etc) as well as common intervention against the new coronavirus spreading.\n",
      "Here we describe a model that emerged from our analysis that was focused on the Renin Angiotensin System (RAS) and the possible routes linking it to the viral infection. because the infection is mediated by the viral receptor on human cell membranes Angiotensin Converting Enzyme (ACE2), which is a key component in RAS signalling. The model depicts the main pathways determining the disease and the molecular framework for its establishment, and can help to shed light on mechanisms involved in the infection. It promptly gives an answer to some of the controversial, and still open, issues concerning predisposing conditions and medical treatments that protect from or favour the severity of the disease (such as the use of ACE inhibitors or ARBs/sartans), or to the sex related biases in the affected population. The model highlights novel opportunities for further investigations, diagnosis and appropriate intervention to understand and fight COVID19.\n",
      "v2: this version does not include Table 1 of the previous version which was copied from http://www.nephjc.com/news/covidace2.\n",
      "I wish to acknowledge: the positive COVID19 patient in Naples that kindly provided information useful to inspire the work. He is still waiting for the test to confirm complete recovery, because of the emergency focused on more urgent issues. It would have been of interest to have a time course monitoring of clinical parameters from him during the disease and soon after, for more understanding; Dr. Antonino Chiusano (radiologist) for useful explanations on data from public radiography images. Dr. Antonio Scala (physicist) for his warm support and for useful suggestions; Dr. Clementina Sansone (biologist) for the evening she spent for a scientific discussion on viruses; Prof. Stefano Mazzoleni for inspiring in me my model based approach for understanding biology.\n",
      "I wish to apologize with my family, for my absence, although being at home because of SARS-CoV2. Hoping that my full day effort in these days will be never for them.\n",
      "3886c0515cabe9ed6e20eeac8cc5e90844d0f053 | The Effect of Stay-at-Home Orders on COVID-19 Infections in the United States: Background In March and April 2020, public health authorities in the United States acted to mitigate transmission of and hospitalizations from the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which causes coronavirus disease 2019 (COVID-19). These actions were not coordinated at the national level, which raises the question of what might have happened if they were. It also creates an opportunity to use spatial and temporal variation to measure their effect with greater accuracy.\n",
      "We combine publicly available data sources on the timing of stay-at-home orders and daily confirmed COVID-19 cases at the county level in the United States ( N = 124,027). We then derive from the classic SIR model a two-way fixed-effects model and apply it to the data with controls for unmeasured differences between counties and over time. This enables us to estimate the effect of stay-at-home orders while accounting for local variation in factors like health systems and demographics, and temporal variation in national mitigation actions, access to tests, or exposure to media reports that could influence the course of the disease.\n",
      "Findings Mean county-level daily growth in COVID-19 infections peaked at 17.2% just before stay-at-home orders were issued. Two way fixed-effects regression estimates suggest that orders were associated with a 3.9 percentage point (95% CI 1.2 to 6.6) reduction in the growth rate after one week and a 6.9 percentage point (2.4 to 11.5) reduction after two weeks. By day 27 the reduction (22.6 percentage points, 14.8 to 30.5) had surpassed the growth at the peak, indicating that growth had turned negative and the number of new daily infections was beginning to decline. A hypothetical national stay-at-home order issued on March 13, 2020 when a national emergency was declared might have reduced cumulative infections by 63.3%, and might have helped to reverse exponential growth in the disease by April 10.\n",
      "Interpretation Although stay-at-home orders impose great costs to society, delayed responses and piecemeal application of these orders generate similar costs without obtaining the full potential benefits suggested by this analysis. The results here suggest that a coordinated 1 nationwide stay-at-home order might have reduced by hundreds of thousands the current number of infections and by tens of thousands the total number of deaths from COVID-19. Future efforts in the United States and elsewhere to control pandemics should coordinate stay-at-home orders at the national level, especially for diseases for which local spread has already occurred and testing availability is delayed. Since stay-at-home orders reduce infection growth rates, early implementation when infection counts are still low would be most beneficial.\n",
      "Funding None.\n",
      "c0b561d09eef775c862a1bee9559f1c707f42e97 | Evidence of economic segregation from mobility lockdown during COVID-19 epidemic: In response to the COVID-19 pandemic, National governments have applied lockdown restrictions to reduce the infection rate. We perform a massive analysis on near real-time Italian data provided by Facebook to investigate how lockdown strategies affect economic conditions of individuals and local governments. We model the change in mobility as an exogenous shock similar to a natural disaster. We identify two ways through which mobility restrictions affect Italian citizens. First, we find that the impact of lockdown is stronger in municipalities with higher fiscal capacity. Second, we find a segregation effect, since mobility restrictions are stronger in municipalities for which inequality is higher and where individuals have lower income per capita.\n",
      "7b72de65011af1999b1beea472cca8d95b25abb8 | CovidSens: A Vision on Reliable Social Sensing based Risk Alerting Systems for COVID-19 Spread: With the spiraling pandemic of the Coronavirus Disease 2019 , it has becoming inherently important to disseminate accurate and timely information about the disease. Due to the ubiquity of Internet connectivity and smart devices, social sensing is emerging as a dynamic sensing paradigm to collect real-time observations from online users. In this vision paper we propose CovidSens, the concept of social-sensing-based risk alerting systems to notify the general public about the COVID-19 spread. The CovidSens concept is motivated by two recent observations: 1) people have been actively sharing their state of health and experience of the COVID-19 via online social media, and 2) official warning channels and news agencies are relatively slower than people reporting their observations and experiences about COVID-19 on social media. We anticipate an unprecedented opportunity to leverage the posts generated by the social media users to build a real-time analytic system for gathering and circulating vital information of the COVID-19 propagation. Specifically, the vision of CovidSens attempts to answer the questions of: how to track the spread of the COVID-19? How to distill reliable information about the disease with the coexistence of prevailing rumors and misinformation in the social media? How to inform the general public about the latest state of the spread timely and effectively and alert them to remain prepared? In this vision paper, we discuss the roles of CovidSens and identify the potential challenges in implementing reliable social-sensing-based risk alerting systems. We envision that approaches originating from multiple disciplines (e.g. estimation theory, machine learning, constrained optimization) can be effective in addressing the challenges. Finally, we outline a few research directions for future work in CovidSens.\n",
      "709f2dbfc5ee48f65ca835ef66ec9e3dc47746b1 | Flattening the Curve: Insights From Queueing Theory: The worldwide outbreak of the coronavirus was first identified in 2019 in Wuhan, China. Since then, the disease has spread worldwide. As it currently spreading in the United States, policy makers, public health officials and citizens are racing to understand the impact of this virus on the United States healthcare system. They fear that the rapid influx of patients will overwhelm the healthcare system leading to unnecessary fatalities. Most countries and states in America have introduced mitigation strategies, * Corresponding Author such as social distancing, to decrease the rate of newly infected people, i.e. flattening the curve.\n",
      "In this paper, we analyze the time evolution of the number of people hospitalized due to the coronavirus using the methods of queueing theory. Given that the rate of new infections varies over time as the pandemic evolves, we model the number of coronavirus patients as a dynamical system based on the theory of infinite server queues with non-stationary Poisson arrival rates. With this model we are able to quantify how flattening the curve affects the peak demand for hospital resources. This allows us to characterize how aggressively society must flatten the curve in order to avoid overwhelming the capacity of healthcare system. We also demonstrate how flattening the curve impacts the elapsed time between the peak rate of hospitalizations and the time of the peak demand for the hospital resources. Finally, we present empirical evidence from China, South Korea, Italy and the United States that supports the insights from the model.\n",
      "cd5d56f9860dabbcb9d1ef512d8009255db7200e | COVID-19 in Africa -outbreak despite interventions?: In Africa, while most countries report some COVID-19 cases, the fraction of reported patients is low, with about 20 000 cases compared to the more than 2.3 million cases reported globally as of April 18, 2020.\n",
      "Few African countries have reported case numbers above one thousand, with South Africa reporting 3 034 cases being hit hardest in Sub-Saharan Africa. Several African countries, especially South Africa, have already taken strong non-pharmaceutical interventions that include physical distancing, restricted economic, educational and leisure activities and reduced human mobility options. The required strengths and overall effectiveness of such interventions, however, are debated because of simultaneous but opposing interests in most African countries: strongly limited health care capacities and testing capabilities largely conflict with pressured national economies and socio-economic hardships on the individual level, limiting compliance to intervention targets. Here we investigate implications of interventions on the COVID-19 outbreak dynamics, focusing on South Africa before and after the national lockdown enacted on March 27, 2020. Our analysis shows that initial exponential growth of existing case numbers is consistent with doubling times of about 2.5 days. After lockdown, the growth remains exponential, now with doubling times of 18 days, but still in contrast to subexponential growth reported for Hubei/China after lockdown. Moreover, a scenario analysis of a computational data-driven agent based mobility model for the Nelson Mandela Bay Municipality (with 1.14 million inhabitants) hints that keeping current levels of intervention measures and compliance until the end of April is of insufficient length and still too weak, too unspecific or too inconsistently complied with to not overload local intensive care capacity. Yet, enduring, slightly stronger, more specific interventions combined with sufficient compliance may constitute a viable option for interventions for regions in South Africa and potentially for large parts of the African continent.\n",
      "d80e378c031591d02641b545a2262e50fe80bb89 | Strong correlations between power-law growth of COVID-19 in four continents and the inefficiency of soft quarantine strategies: In this work we analyse the growth of the cumulative number of confirmed infected cases by the COVID-19 until March 27 th , 2020, from countries of Asia, Europe, North and South America. Our results show (i) that power-law growth is observed for all countries; (ii) by using the distance correlation, that the power-law curves between countries are statistically highly correlated, suggesting the universality of such curves around the World; and (iii) that soft quarantine strategies are inefficient to flatten the growth curves. Furthermore, we present a model and strategies which allow the government to reach the flattening of the power-law curves. We found that, besides the social distance of individuals, of well known relevance, the strategy of identifying and isolating infected individuals in a large daily rate, can help to flatten the power-laws. These are essentially the strategies used in the Republic of Korea. The high correlation between the power-law curves of different countries strongly indicate that the government containment measures can be applied with success around the whole World. These measures must be scathing and applied as soon as possible.\n",
      "067e53fab4bdec6e6dd07365d38f7e8b3be9c688 | Using generalized logistics regression to forecast population infected by Covid-19: In this work, a proposal to forecast the populations using generalized logistics regression curve fitting is presented. This type of curve is used to study population growth, in this case population of people infected with the Covid-19 virus; and it can also be used to approximate the survival curve used in actuarial and similar studies.\n",
      "fa4c0136b5f96f629bab51b569a3eb4f24b0db0f | Total Variation Regularization for Compartmental Epidemic Models with Time-varying Dynamics: Traditional methods to infer compartmental epidemic models with time-varying dynamics can only capture continuous changes in the dynamic. However, many changes are discontinuous due to sudden interventions, such as city lockdown and opening of field hospitals. To model the discontinuities, this study introduces the tool of total variation regularization, which regulates the temporal changes of the dynamic parameters, such as the transmission rate. To recover the ground truth dynamic, this study designs a novel yet straightforward optimization algorithm, dubbed iterative Nelder-Mead, which repeatedly applies the Nelder-Mead algorithm. Experiments on the simulated data show that the proposed approach can qualitatively reproduce the discontinuities of the underlying dynamics. To extend this research to real data as well as to help researchers worldwide to fight against COVID-19, the author releases his research platform as an open-source package.\n",
      "2c837018d332088b3b45f0b6189d1df3cb53064d | USING ALTMETRICS FOR DETECTING IMPACTFUL RESEARCH IN QUASI-ZERO-DAY TIME-WINDOWS: THE CASE OF COVID-19 A PREPRINT: \n",
      "b923a8fc7d406d0f149ecd5293d1a72dec7ee526 | An alternating lock-down strategy for sustainable mitigation of COVID-19: Lacking a drug or vaccine, our current strategy to contain the COVID-19 pandemic is by means of social distancing, specifically mobility restrictions and lock-downs. Such measures impose a hurtful toll on the economy, and are difficult to sustain for extended periods. The challenge is that selective isolation of the sick, an often viable and effective strategy, is insufficient against COVID-19, due to its relatively long incubation period, in which exposed individuals experience no symptoms, but still contribute to the spread. Here we propose an alternating lock-down strategy, in which at every instance, half of the population remains under lock-down while the other half continues to be active, maintaining a routine of weekly succession between activity and lock-down. All symptomatic individuals continue to remain in isolation. Under this regime, if an individual was exposed during their active week, by the time they complete their lock-down they will already begin to exhibit symptoms. Hence this strategy isolates the majority of exposed individuals during their asymptomatic phase. We find that this strategy not only overcomes the pandemic, but also allows for some level of flexibility, withstanding a fraction of defectors or essential workers that remain continuously active. We examine our strategy based on current epidemiological models with parameters relevant for COVID-19. We wish, however, following this communication, to further test and fine-tune our scheme based more refined data, and assess its actual effectiveness.\n",
      "Note from the authors. In light of the imminence of the matter, we have decided to publish this report, even in its preliminary form, forgoing the scientific instinct of scrutiny and reservedness. We will continue to refine and retest our results and update this report on the go. We also welcome feedback, comments, questions or advice that can help further test or improve our proposed strategy.\n",
      "As we battle the virulent spread of COVID-19 we seek efficient strategies to mitigate its global impact. Lacking therapeutic interventions, such as drugs or vaccines, we resort to social distancing, aimed primarily at lowering the reproduction rate R 0 and flattening the curve, i.e. reducing the total number of infected individuals, while spreading their infection over an extended period [1-3]. Such measures are designed to avoid shocking the health-care system, reducing the medical burden, and slowing its onset, hence keeping it within the bounds of the system's capacity. To achieve this many countries have imposed social restrictions [3], from complete lock-downs, to severe mobility constraints, indeed, slowing down the viral propagation, but at the same time, taking a sever toll on social and economic stability. Most current projections on COVID-19 indicate that such social distancing policies must be put in place for extended periods (typically months) to avoid reemergence of the epidemic once lifted [4] . This, however, may be unsustainable, as individual social and economic needs will, at some point surpass the perceived risk of the pandemic.\n",
      "Another challenge, specific to COVID-19, is its incubation period, estimated at ∼ 5 days on average\n",
      "bddf336f4c53ce53c22a54de37aa00c8b723a291 | Weather-inspired ensemble-based probabilistic prediction of COVID-19: The objective of this work is to predict the spread of COVID-19 starting from observed data, using a forecast method inspired by probabilistic weather prediction systems operational today.\n",
      "Results show that this method works well for China: on day 25 we could have predicted well the outcome for the next 35 days. The same method has been applied to Italy and South Korea, and forecasts for the forthcoming weeks are included in this work. For Italy, forecasts based on data collected up to today (24 March) indicate that number of observed cases could grow from the current value of 69,176, to between 101k-180k, with a 50% probability of being between 110k-135k. For South Korea, it suggests that the number of observed cases could grow from the current value of 9,018 (as of the 23rd of March), to values between 8,500 and 9,300, with a 50% probability of being between 8,700 and 8,900.\n",
      "We conclude by suggesting that probabilistic disease prediction systems are possible and could be developed following key ideas and methods from weather forecasting. Having access to skilful daily updated forecasts could help taking better informed decisions on how to manage the spread of diseases such as COVID-19.\n",
      "f0d58f10263d2a5a7c87f0a73c0f38b1af2e67be | Optimal resource diffusion for suppressing disease spreading in multiplex networks: Resource diffusion is an ubiquitous phenomenon, but how it impacts epidemic spreading has received little study. We propose a model that couples epidemic spreading and resource diffusion in multiplex networks. The spread of disease in a physical contact layer and the recovery of the infected nodes are both strongly dependent upon resources supplied by their counterparts in the social layer. The generation and diffusion of resources in the social layer are in turn strongly dependent upon the state of the nodes in the physical contact layer. Resources diffuse preferentially or randomly in this model. To quantify the degree of preferential diffusion, a bias parameter that controls the resource diffusion is proposed. We conduct extensive simulations and find that the preferential resource diffusion can change phase transition type of the fraction of infected nodes. When the degree of interlayer correlation is below a critical value, increasing the bias parameter changes the phase transition from double continuous to single continuous. When the degree of interlayer correlation is above a critical value, the phase transition changes from multiple continuous to first discontinuous and then to hybrid. We find hysteresis loops in the phase transition. We also find that there is an optimal resource strategy at each fixed degree of interlayer correlation where the threshold reaches a maximum and under which the disease can be maximally suppressed. In addition, the optimal controlling parameter increases as the degree of inter-layer correlation increases.\n",
      "f32610fc5bca19a9e0890eb1e868d057397762ad | Recognition of potential Covid-19 drug treatments through study of existing protein-drug structures: an analysis of thermodynamically active residues: We report results of our study of approved drugs as potential treatments for COVID-19 based on the application of various bioinformatics predictive methods. The drugs studied include hydroxychloroquine, ivermectin, remdesivir and α-difluoromethylornithine (DMFO). Our results indicate that these small drug molecules selectively bind to thermodynamically active residues on protein surfaces, and that some prefer hydrophobic over other active sites. Our approach is not restricted to viruses and can facilitate rational drug design, as well as improve our understanding of molecular interactions, in general.\n",
      "855e9a1e74eceea68ad316023622114cfadf026b | In Silico Investigations on the Potential Inhibitors for COVID-19 Protease *: A novel strain of coronavirus, namely, COVID-19 has been identified in Wuhan city of China in December 2019. There are no specific therapies available and investigations regarding the treatment of the COVID-19 are still lacking. This prompted us to perform a preliminary in silico study on the COVID-19 protease with anti-malarial compounds in the search of potential inhibitor. We have calculated log P and log S values in addition to molecular docking and PASS predictions. Among the seven studied compounds, mepacrine appears as the potential inhibitor of the COVID-19 followed by chloroquine, hydroxychloroquine and phomarin. Therefore, these anti-malarial drugs may be potential drug candidate for the treatment of this novel coronavirus.\n",
      "A detailed analysis on these inhibitors is currently in progress and clinical studies are invited to investigate their potential medicinal use for the COVID-19.\n",
      "4cc3622caaba4cf5c90c64924ace1d2c3e0b40e1 | An Ensemble Approach to Predicting the Impact of Vaccination on Rotavirus Disease in Niger: Recently developed vaccines provide a new way of controlling rotavirus in sub-Saharan Africa.\n",
      "Models for the transmission dynamics of rotavirus are critical both for estimating current burden from imperfect surveillance and for assessing potential effects of vaccine intervention strategies.\n",
      "We examine rotavirus infection in the Maradi area in southern Niger using hospital surveillance data provided by Epicentre collected over two years. Additionally, a cluster survey of households in the region allows us to estimate the proportion of children with diarrhea who consulted at a health structure. Model fit and future projections are necessarily particular to a given model; thus, where there are competing models for the underlying epidemiology an ensemble approach can account for that uncertainty. We compare our results across several variants of Susceptible-Infectious-Recovered (SIR) compartmental models to quantify the impact of modeling assumptions on our estimates. Model-specific parameters are estimated by Bayesian inference using Markov chain Monte Carlo. We then use Bayesian model averaging to generate ensemble estimates of the current dynamics, including estimates of R0, the burden of infection in the region, as well as the impact of vaccination on both the short-term dynamics and the long-term reduction of rotavirus incidence under varying levels of coverage. The ensemble of models predicts that the current burden of severe rotavirus disease is 2.9 to 4.1% of the population each year and that a 2-dose vaccine schedule achieving 70% coverage could reduce burden by 37-43%.\n"
     ]
    }
   ],
   "source": [
    "res = es.search(\n",
    "    index=\"covid19_papers\", \n",
    "    body={\n",
    "        \"from\": 0,\n",
    "        \"size\": 20,\n",
    "        \"query\": {\"match_all\": {}}\n",
    "    }\n",
    ")\n",
    "for hit in res['hits']['hits']:\n",
    "    print(\"%(paper_id)s | %(title)s: %(abstract)s\" % hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
